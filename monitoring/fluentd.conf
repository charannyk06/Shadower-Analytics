# Fluentd Configuration for Analytics Service Log Aggregation
# This configuration collects, processes, and forwards logs to Elasticsearch

# Input: Receive logs from applications via forward protocol
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Input: Tail Docker container logs (if running in Docker)
<source>
  @type tail
  @id analytics_backend_logs
  path /var/log/containers/analytics-backend-*.log
  pos_file /var/log/fluentd/analytics-backend.pos
  tag analytics.backend.*
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Filter: Add common fields to all analytics logs
<filter analytics.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
    service "analytics"
    log_source "fluentd"
  </record>
</filter>

# Filter: Parse request logs and extract useful information
<filter analytics.backend.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

# Filter: Add geo-location data for client IPs
<filter analytics.backend.**>
  @type geoip
  geoip_lookup_keys client_ip
  <record>
    location ${city.names.en["client_ip"]}
    country ${country.iso_code["client_ip"]}
    latitude ${location.latitude["client_ip"]}
    longitude ${location.longitude["client_ip"]}
  </record>
  skip_adding_null_record true
</filter>

# Filter: Throttle error logs to prevent spam
<filter analytics.error>
  @type throttle
  group_key workspace_id
  group_bucket_period_s 60
  group_bucket_limit 10
  group_drop_logs true
</filter>

# Filter: Detect and flag anomalous log patterns
<filter analytics.backend.**>
  @type detect_exceptions
  languages python
  multiline_flush_interval 0.1
</filter>

# Filter: Redact sensitive information
<filter analytics.**>
  @type record_modifier
  <record>
    # Remove sensitive fields
    password ""
    api_key ""
    secret ""
    token ""
  </record>
  remove_keys password,api_key,secret,token
</filter>

# Filter: Add log level severity
<filter analytics.**>
  @type record_transformer
  enable_ruby true
  <record>
    severity ${record["level"] || "INFO"}
    severity_number ${
      case record["level"]
      when "DEBUG" then 1
      when "INFO" then 2
      when "WARNING", "WARN" then 3
      when "ERROR" then 4
      when "CRITICAL", "FATAL" then 5
      else 0
      end
    }
  </record>
</filter>

# Output: Send to Elasticsearch
<match analytics.**>
  @type elasticsearch
  @id analytics_elasticsearch

  # Elasticsearch configuration
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER']}"
  password "#{ENV['ELASTICSEARCH_PASSWORD']}"

  # Index configuration
  logstash_format true
  logstash_prefix analytics
  logstash_dateformat %Y.%m.%d

  # Index lifecycle
  ilm_policy_id analytics_logs
  enable_ilm true

  # Document settings
  include_timestamp true
  time_key @timestamp

  # Performance settings
  <buffer>
    @type memory
    flush_interval 10s
    flush_thread_count 2
    chunk_limit_size 5M
    queue_limit_length 32
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 60s
    retry_timeout 1h
    overflow_action drop_oldest_chunk
  </buffer>

  # Error handling
  <secondary>
    @type file
    path /var/log/fluentd/failed_records
    compress gzip
  </secondary>
</match>

# Output: Send critical errors to separate index
<match analytics.error>
  @type elasticsearch
  @id analytics_errors

  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER']}"
  password "#{ENV['ELASTICSEARCH_PASSWORD']}"

  # Use separate index for errors
  logstash_format true
  logstash_prefix analytics-errors
  logstash_dateformat %Y.%m.%d

  <buffer>
    @type memory
    flush_interval 5s
    flush_thread_count 1
  </buffer>
</match>

# Output: Send metrics logs to separate pipeline
<match analytics.metrics>
  @type copy

  # Send to Elasticsearch
  <store>
    @type elasticsearch
    host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
    port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
    logstash_format true
    logstash_prefix analytics-metrics
  </store>

  # Also send to stdout for debugging
  <store>
    @type stdout
  </store>
</match>

# Output: Performance monitoring
<match analytics.performance>
  @type elasticsearch
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
  logstash_format true
  logstash_prefix analytics-performance

  <buffer>
    @type memory
    flush_interval 30s
  </buffer>
</match>

# Catch-all for unmatched logs
<match **>
  @type stdout
  <format>
    @type json
  </format>
</match>

# Prometheus metrics for Fluentd itself
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

<source>
  @type prometheus_monitor
  <labels>
    host ${hostname}
  </labels>
</source>

<source>
  @type prometheus_output_monitor
  <labels>
    host ${hostname}
  </labels>
</source>
